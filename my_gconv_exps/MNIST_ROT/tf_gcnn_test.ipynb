{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorlayer as tl\n",
    "import tensorlayer.layers as tly\n",
    "from groupy.gconv.tensorflow_gconv.splitgconv2d import gconv2d, gconv2d_util\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gconv_bn_act(x, gconv_type, C_in, C_out, ksize=3, padding='VALID', \n",
    "                 bn=True, is_train=True, act=tf.nn.relu, \n",
    "                 name='gconv_bn_act', reuse=False):\n",
    "    if gconv_type == 'Z2_to_P4':\n",
    "        h_in = 'Z2'\n",
    "        h_out = 'C4'\n",
    "    elif gconv_type == 'Z2_to_P4M':\n",
    "        h_in = 'Z2'\n",
    "        h_out = 'D4'\n",
    "    elif gconv_type == 'P4_to_P4':\n",
    "        h_in = 'C4'\n",
    "        h_out = 'C4'\n",
    "    elif gconv_type == 'P4M_to_P4M':\n",
    "        h_in = 'D4'\n",
    "        h_out = 'D4'\n",
    "    else:\n",
    "        raise NotImplemented('Unsupported gconv_type: {}!'.format(gconv_type))\n",
    "    with tf.variable_scope(name, reuse=reuse) as vs:\n",
    "        gconv_indices, gconv_shape_info, w_shape = gconv2d_util(\n",
    "                h_input=h_in, h_output=h_out, in_channels=C_in, out_channels=C_out, ksize=ksize)\n",
    "        w = tf.Variable(tf.truncated_normal(w_shape, stddev=1.))\n",
    "        x = gconv2d(input=x, filter=w, strides=[1, 1, 1, 1], padding=padding,\n",
    "                gconv_indices=gconv_indices, gconv_shape_info=gconv_shape_info)\n",
    "        if bn:\n",
    "            x = tf.layers.batch_normalization(x, training=is_train)\n",
    "        return act(x)\n",
    "\n",
    "def p4m_cnn(x, y_ , is_train, kernel_size=3, stride=1, drop_rate=0.3, reuse=False):\n",
    "  '''\n",
    "  x: NHWC\n",
    "  y_: N\n",
    "  '''\n",
    "  C_out = 10\n",
    "  with tf.variable_scope(\"P4MCNN\", reuse=reuse) as vs:\n",
    "    x = gconv_bn_act(x, gconv_type='Z2_to_P4M', C_in=1, C_out=C_out, is_train=is_train,\n",
    "                reuse=reuse, name='Z2_to_P4M_gconv_bn_act1') # l1\n",
    "    x = tf.layers.dropout(inputs=x, rate=drop_rate,\n",
    "                 training=is_train, name='drop1')\n",
    "    print(x.get_shape().as_list()) # [None, 26, 26, 80]\n",
    "\n",
    "    x = gconv_bn_act(x, gconv_type='P4M_to_P4M', C_in=C_out, C_out=C_out, is_train=is_train,\n",
    "                reuse=reuse, name='P4M_to_P4M_gconv_bn_act2') # l2\n",
    "    print(x.get_shape().as_list()) # [None, 24, 24, 80]\n",
    "    xs = x.get_shape().as_list()\n",
    "    x = tf.reshape(x, shape=[-1, xs[1], xs[2], 8, xs[3]//8])\n",
    "    print(x.get_shape().as_list()) # [None, 24, 24, 8, 10]\n",
    "    x = tf.reduce_max(x, axis=(-2), keep_dims=False, name='reduce_max_l2')\n",
    "    print(x.get_shape().as_list()) # [None, 24, 24, 10]\n",
    "    x = tf.layers.max_pooling2d(x, pool_size=2, strides=2,\n",
    "                    padding='valid',\n",
    "                    data_format='channels_last', name='max_pool2')\n",
    "    print(x.get_shape().as_list()) # [None, 12, 12, 10]\n",
    "    x = gconv_bn_act(x, gconv_type='Z2_to_P4M', C_in=C_out, C_out=C_out, is_train=is_train,\n",
    "                reuse=reuse, name='Z2_to_P4M_gconv_bn_act3') # l3\n",
    "    x = tf.layers.dropout(inputs=x, rate=drop_rate,\n",
    "                training=is_train, name='drop3')\n",
    "    print(x.get_shape().as_list()) # [None, 10, 10, 80]\n",
    "    x = gconv_bn_act(x, gconv_type='P4M_to_P4M', C_in=C_out, C_out=C_out, is_train=is_train,\n",
    "                reuse=reuse, name='P4M_to_P4M_gconv_bn_act4') # l4\n",
    "    x = tf.layers.dropout(inputs=x, rate=drop_rate,\n",
    "                training=is_train, name='drop4')\n",
    "    print(x.get_shape().as_list()) # [None, 8, 8, 80]\n",
    "    x = gconv_bn_act(x, gconv_type='P4M_to_P4M', C_in=C_out, C_out=C_out, is_train=is_train,\n",
    "                reuse=reuse, name='P4M_to_P4M_gconv_bn_act5') # l5\n",
    "    x = tf.layers.dropout(inputs=x, rate=drop_rate,\n",
    "                training=is_train, name='drop5')\n",
    "    print(x.get_shape().as_list()) # [None, 6, 6, 80]\n",
    "    x = gconv_bn_act(x, gconv_type='P4M_to_P4M', C_in=C_out, C_out=C_out, is_train=is_train,\n",
    "                reuse=reuse, name='P4M_to_P4M_gconv_bn_act6') # l6\n",
    "    x = tf.layers.dropout(inputs=x, rate=drop_rate,\n",
    "                training=is_train, name='drop6')\n",
    "    print(x.get_shape().as_list()) # [None, 4, 4, 80]\n",
    "    x = gconv_bn_act(x, gconv_type='P4M_to_P4M', C_in=C_out, C_out=C_out, is_train=is_train,\n",
    "                padding=\"VALID\",\n",
    "                bn=False,\n",
    "                act=tf.identity, reuse=reuse, name='P4M_to_P4M_gconv7',) # l7\n",
    "    print(x.get_shape().as_list()) # [None, 2, 2, 80]\n",
    "\n",
    "    # take max value of (H,W) dimensions\n",
    "    # NHWC --> Nx10\n",
    "    xs = x.get_shape().as_list() \n",
    "    x = tf.reshape(x, shape=[-1, xs[1], xs[2], 8, xs[3]//8])\n",
    "    print(x.get_shape().as_list()) # [None, 2, 2, 8, 10]\n",
    "    logits = tf.reduce_max(x, axis=(1, 2, 3), keep_dims=False, name='reduce_max_l7')\n",
    "    print(logits.get_shape().as_list()) # [None, 10]\n",
    "  return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, 28, 28, 1], 'x')\n",
    "y_ = tf.placeholder(tf.int32, [None], 'y_')\n",
    "is_train = tf.placeholder(tf.bool, name='is_train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 26, 26, 80]\n",
      "[None, 24, 24, 80]\n",
      "[None, 24, 24, 8, 10]\n",
      "[None, 24, 24, 10]\n",
      "[None, 12, 12, 10]\n",
      "[None, 10, 10, 80]\n",
      "[None, 8, 8, 80]\n",
      "[None, 6, 6, 80]\n",
      "[None, 4, 4, 80]\n",
      "[None, 2, 2, 80]\n",
      "[None, 2, 2, 8, 10]\n",
      "[None, 10]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'P4MCNN/reduce_max_l7:0' shape=(?, 10) dtype=float32>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p4m_cnn(x,y_, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def p4_cnn(x, y_ , is_train, kernel_size=3, stride=1, drop_rate=0.3, reuse=False):\n",
    "  '''\n",
    "  x: NHWC\n",
    "  y_: N\n",
    "  '''\n",
    "  C_out = 10\n",
    "  with tf.variable_scope(\"P4CNN\", reuse=reuse) as vs:\n",
    "    x = gconv_bn_act(x, gconv_type='Z2_to_P4', C_in=1, C_out=C_out, is_train=is_train,\n",
    "                reuse=reuse, name='Z2_to_P4_gconv_bn_act1') # l1\n",
    "    # drop1 = tf.layers.dropout(inputs=conv_bn_act_1, rate=drop_rate,\n",
    "    #             training=is_train, name='drop1')\n",
    "    print(x.get_shape().as_list()) # [None, 26, 26, 40]\n",
    "\n",
    "    x = gconv_bn_act(x, gconv_type='P4_to_P4', C_in=C_out, C_out=C_out, is_train=is_train,\n",
    "                reuse=reuse, name='P4_to_P4_gconv_bn_act2') # l2\n",
    "    print(x.get_shape().as_list()) # [None, 24, 24, 40]\n",
    "    x = tf.layers.max_pooling2d(x, pool_size=2, strides=2,\n",
    "                    padding='valid',\n",
    "                    data_format='channels_last', name='max_pool2')\n",
    "    print(x.get_shape().as_list()) # [None, 12, 12, 40]\n",
    "    x = gconv_bn_act(x, gconv_type='P4_to_P4', C_in=C_out, C_out=C_out, is_train=is_train,\n",
    "                reuse=reuse, name='P4_to_P4_gconv_bn_act3') # l3\n",
    "    # x = tf.layers.dropout(inputs=x, rate=drop_rate,\n",
    "    #             training=is_train, name='drop3')\n",
    "    print(x.get_shape().as_list()) # [None, 10, 10, 40]\n",
    "    x = gconv_bn_act(x, gconv_type='P4_to_P4', C_in=C_out, C_out=C_out, is_train=is_train,\n",
    "                reuse=reuse, name='P4_to_P4_gconv_bn_act4') # l4\n",
    "    # x = tf.layers.dropout(inputs=x, rate=drop_rate,\n",
    "    #             training=is_train, name='drop4')\n",
    "    print(x.get_shape().as_list()) # [None, 8, 8, 40]\n",
    "    x = gconv_bn_act(x, gconv_type='P4_to_P4', C_in=C_out, C_out=C_out, is_train=is_train,\n",
    "                reuse=reuse, name='P4_to_P4_gconv_bn_act5') # l5\n",
    "    # x = tf.layers.dropout(inputs=x, rate=drop_rate,\n",
    "    #             training=is_train, name='drop5')\n",
    "    print(x.get_shape().as_list()) # [None, 6, 6, 40]\n",
    "    x = gconv_bn_act(x, gconv_type='P4_to_P4', C_in=C_out, C_out=C_out, is_train=is_train,\n",
    "                reuse=reuse, name='P4_to_P4_gconv_bn_act6') # l6\n",
    "    # x = tf.layers.dropout(inputs=x, rate=drop_rate,\n",
    "    #             training=is_train, name='drop6')\n",
    "    print(x.get_shape().as_list()) # [None, 4, 4, 40]\n",
    "    x = gconv_bn_act(x, gconv_type='P4_to_P4', C_in=C_out, C_out=C_out, is_train=is_train,\n",
    "                padding=\"VALID\",\n",
    "                bn=False,\n",
    "                act=tf.identity, reuse=reuse, name='P4_to_P4_gconv7',) # l7\n",
    "    print(x.get_shape().as_list()) # [None, 2, 2, 40]\n",
    "\n",
    "    # take max value of (H,W) dimensions\n",
    "    # NHWC --> Nx10\n",
    "    xs = x.get_shape().as_list() \n",
    "    x = tf.reshape(x, shape=[-1, xs[1], xs[2], 4, xs[3]//4])\n",
    "    print(x.get_shape().as_list()) # [None, 2, 2, 4, 10]\n",
    "    logits = tf.reduce_max(x, axis=(1, 2, 3), keep_dims=False, name='reduce_max1')\n",
    "    # print(logits.get_shape().as_list()) # [None, 10]\n",
    "  return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Construct graph\n",
    "x = tf.placeholder(tf.float32, [None, 9, 9, 3])\n",
    "\n",
    "gconv_indices, gconv_shape_info, w_shape = gconv2d_util(\n",
    "    h_input='Z2', h_output='D4', in_channels=3, out_channels=64, ksize=3)\n",
    "w = tf.Variable(tf.truncated_normal(w_shape, stddev=1.))\n",
    "y = gconv2d(input=x, filter=w, strides=[1, 1, 1, 1], padding='SAME',\n",
    "            gconv_indices=gconv_indices, gconv_shape_info=gconv_shape_info)\n",
    "print(y.shape)\n",
    "\n",
    "gconv_indices, gconv_shape_info, w_shape = gconv2d_util(\n",
    "    h_input='D4', h_output='D4', in_channels=64, out_channels=64, ksize=3)\n",
    "w = tf.Variable(tf.truncated_normal(w_shape, stddev=1.))\n",
    "y = gconv2d(input=y, filter=w, strides=[1, 1, 1, 1], padding='SAME',\n",
    "            gconv_indices=gconv_indices, gconv_shape_info=gconv_shape_info)\n",
    "print(y.shape)\n",
    "# Compute\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "y = sess.run(y, feed_dict={x: np.random.randn(10, 9, 9, 3)})\n",
    "sess.close()\n",
    "\n",
    "print(y.shape)  # (10, 9, 9, 512) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import chainer.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import chainer.functions as F\n",
    "from chainer import Chain\n",
    "\n",
    "class ConvBNAct(Chain):\n",
    "\n",
    "    def __init__(self,\n",
    "                 conv,\n",
    "                 bn=True,\n",
    "                 act=F.relu):\n",
    "        super(ConvBNAct, self).__init__(conv=conv)\n",
    "\n",
    "        if bn:\n",
    "            out_channels = self.conv.W.data.shape[0]\n",
    "            self.add_link('bn', F.BatchNormalization(out_channels))\n",
    "        else:\n",
    "            self.bn = None\n",
    "\n",
    "        self.act = act\n",
    "\n",
    "    def __call__(self, x, train, finetune):\n",
    "\n",
    "        y = self.conv(x)\n",
    "\n",
    "        if self.bn:\n",
    "            y = self.bn(y, test=not train, finetune=finetune)\n",
    "        if self.act:\n",
    "            y = self.act(y)\n",
    "\n",
    "        return y\n",
    "\n",
    "\n",
    "class Z2CNN(Chain):\n",
    "\n",
    "    def __init__(self):\n",
    "        ksize = 3\n",
    "        bn = True\n",
    "        act = F.relu\n",
    "        self.dr = 0.3\n",
    "        super(Z2CNN, self).__init__(\n",
    "\n",
    "            l1=ConvBNAct(\n",
    "                conv=F.Convolution2D(in_channels=1, out_channels=20, ksize=ksize, stride=1, pad=0),\n",
    "                bn=bn,\n",
    "                act=act\n",
    "            ),\n",
    "\n",
    "            l2=ConvBNAct(\n",
    "                conv=F.Convolution2D(in_channels=20, out_channels=20, ksize=ksize, stride=1, pad=0),\n",
    "                bn=bn,\n",
    "                act=act\n",
    "            ),\n",
    "\n",
    "            l3=ConvBNAct(\n",
    "                conv=F.Convolution2D(in_channels=20, out_channels=20, ksize=ksize, stride=1, pad=0),\n",
    "                bn=bn,\n",
    "                act=act\n",
    "            ),\n",
    "\n",
    "            l4=ConvBNAct(\n",
    "                conv=F.Convolution2D(in_channels=20, out_channels=20, ksize=ksize, stride=1, pad=0),\n",
    "                bn=bn,\n",
    "                act=act\n",
    "            ),\n",
    "\n",
    "            l5=ConvBNAct(\n",
    "                conv=F.Convolution2D(in_channels=20, out_channels=20, ksize=ksize, stride=1, pad=0),\n",
    "                bn=bn,\n",
    "                act=act\n",
    "            ),\n",
    "\n",
    "            l6=ConvBNAct(\n",
    "                conv=F.Convolution2D(in_channels=20, out_channels=20, ksize=ksize, stride=1, pad=0),\n",
    "                bn=bn,\n",
    "                act=act\n",
    "            ),\n",
    "\n",
    "            top=F.Convolution2D(in_channels=20, out_channels=10, ksize=4, stride=1, pad=0),\n",
    "        )\n",
    "\n",
    "    def __call__(self, x, t, train=True, finetune=False):\n",
    "\n",
    "        h = self.l1(x, train, finetune)\n",
    "        h = F.dropout(h, self.dr, train)\n",
    "        h = self.l2(h, train, finetune)\n",
    "\n",
    "        h = F.max_pooling_2d(h, ksize=2, stride=2, pad=0, cover_all=True, use_cudnn=True)\n",
    "\n",
    "        h = self.l3(h, train, finetune)\n",
    "        h = F.dropout(h, self.dr, train)\n",
    "        h = self.l4(h, train, finetune)\n",
    "        h = F.dropout(h, self.dr, train)\n",
    "        h = self.l5(h, train, finetune)\n",
    "        h = F.dropout(h, self.dr, train)\n",
    "        h = self.l6(h, train, finetune)\n",
    "        h = F.dropout(h, self.dr, train)\n",
    "\n",
    "        h = self.top(h)\n",
    "\n",
    "        h = F.max(h, axis=-1, keepdims=False)\n",
    "        h = F.max(h, axis=-1, keepdims=False)\n",
    "\n",
    "        return F.softmax_cross_entropy(h, t), F.accuracy(h, t)\n",
    "\n",
    "    def start_finetuning(self):\n",
    "        for c in self.children():\n",
    "            if isinstance(c, ConvBNAct):\n",
    "                if c.bn:\n",
    "                    c.bn.start_finetuning()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function join in module posixpath:\n",
      "\n",
      "join(a, *p)\n",
      "    Join two or more pathname components, inserting '/' as needed.\n",
      "    If any component is an absolute path, all previous path components\n",
      "    will be discarded.  An empty last part will result in a path that\n",
      "    ends with a separator.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(os.path.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
